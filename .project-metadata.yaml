name: Fine-Tuning a foundation model for multiple tasks (with QLoRA)
description: |
  This AMP demonstrates how PEFT and other fine-tuning optimization techniques can be used for efficient and effective customization of an existing LLM to perform new tasks.
author: Cloudera Inc.
specification_version: 1.0
prototype_version: 1.0
date: "2023-07-22"

environment_variables:
  NUM_GPU_WORKERS:
    default: "2"
    description: >-
      The total number of GPUs that will be used for the optional distributed fine-tuning jobs. If 1 is set, fine-tuning will happen on a single container only without distribution. Default: 2
  CUSTOM_LORA_ADAPTERS_DIR:
    default: "amp_adapters_custom"
    description: >-
      The directory containing the reproduced LoRA adapters created by the fine-tuning jobs in this project. Also the location to look for any custom LoRA adapters.

runtimes:
  - editor: PBJ Workbench
    kernel: Python 3.9
    edition: Nvidia GPU

tasks:
  - type: run_session
    name: Validate GPU Availibility in this workspace
    script: amp_0_session-configuration-validation/check_gpu_resources.py
    short_summary: Check for GPU availibility. 
    long_summary: Check GPUs are enabled on this workspace and are currently schedulable.
    kernel: python3
    cpu: 2
    memory: 4

  - type: run_session
    name: Install Dependencies
    script: amp_1_session-install-deps/install-requirements.py
    short_summary: Install Dependencies
    kernel: python3
    cpu: 2
    memory: 8
  
  - type: run_session
    name: Validate GPU CUDA Capability
    script: amp_2_session-resource-validation/check_gpu_capability.py
    short_summary: Check for GPU capability. 
    long_summary: Check GPU device supports the CUDA capabilities required.
    kernel: python3
    cpu: 2
    memory: 4
    gpu: 1

  - type: create_job
    name: Job for distributed fine-tuning on Instruction Dataset
    short_summary: Create Job for fine-tuning on Instruction Dataset
    entity_label: fine_tune_instruct
    script: amp_3_job_fine_tune/job-instruction.py
    arguments: None
    long_summary: Create job to fine-tune an LLM on a subset of the teknium/GPTeacher-General-Instruct dataset. This job will not be automatically launched during AMP startup. A sample LoRA adapter fine-tuned with this job script and dataset is included in the git repository.
    cpu: 2
    memory: 8
    gpu: 1
    environment:
      TASK_TYPE: CREATE/RUN_JOB

  - type: create_job
    name: Job for distributed fine-tuning on SQL Generation Dataset
    short_summary: Create Job for fine-tuning on SQL Generation Dataset
    entity_label: fine_tune_sql
    script: amp_3_job_fine_tune/job-sql.py
    arguments: None
    long_summary: Create job to fine-tune an LLM on the philschmid sql-create-context-copy dataset. This job will not be automatically launched during AMP startup. A sample LoRA adapter fine-tuned with this job script and dataset is included in the git repository.
    cpu: 2
    memory: 8
    gpu: 1
    environment:
      TASK_TYPE: CREATE/RUN_JOB

  - type: create_job
    name: Job for distributed fine-tuning on Detox Dataset
    short_summary: Create Job for fine-tuning on Detox Dataset
    entity_label: fine_tune_detox
    script: amp_3_job_fine_tune/job-detox.py
    arguments: None
    long_summary: Create job to fine-tune an LLM on the s-nlp/paradetox dataset. This job will not be automatically launched during AMP startup. A sample LoRA adapter fine-tuned with this job script and dataset is included in the git repository.
    cpu: 2
    memory: 8
    gpu: 1
    environment:
      TASK_TYPE: CREATE/RUN_JOB
  
  - type: start_application
    name: CML LLM Fine-Tuned Task Explorer
    subdomain: cml-task
    script: amp_4_app-task-explorer/task-explorer.py
    long_summary: This application requires an available GPU to run the LLM model and LoRA adapters.
    cpu: 2
    memory: 16
    gpu: 1
    environment_variables:
      TASK_TYPE: START_APPLICATION