{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97044c1b-87ab-41fc-b459-84f8b14e0b1d",
   "metadata": {},
   "source": [
    "# Accelerate Distributed Fine-Tuning a foundation model for multiple tasks (with QLoRA)\n",
    "Using the huggingface accelerate API and CML Workers, show how to set up configurations to use multiple CML workers with GPU to perform distributed training.\n",
    "\n",
    "The following notebook is an example of performing the bundled QLoRA fine-tuning on an LLM using an instruction-following dataset distributed across multiple CML Workers. This script produces the same instruction-following adapter as shown in the amp_adapters_prebuilt directory and the CML Job \"Job for fine-tuning on Instruction Dataset\"\n",
    "\n",
    "Requirements:\n",
    "- Notebook Session:\n",
    "  - 2 CPU / 8 MEM / 1 GPU\n",
    "- GPUs:\n",
    "This notebook requires access within this CML workspace for a total of 2 GPUs.\n",
    "  - 1 for this Notebook Session (described above)\n",
    "  - 1 for the spawned CML Worker.\n",
    "- Runtime:\n",
    "  - JupyterLab - Python 3.9 - Nvidia GPU - 2023.05\n",
    "\n",
    "Note: This executes fine-tuning code defined in amp_3_job_fine_tune/distributed_peft_scripts. See the implementation README in amp_3_job_fine_tune/distributed_peft_scripts for a description of the fine-tuning code using huggingface transformers/trl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef0a24-ba43-449f-98cf-e22a05062b0e",
   "metadata": {},
   "source": [
    "### Set Training Script Path\n",
    "This is the training script that will be distributed. The script itself can be run standalone or distributed with accelerate thanks to huggingface transformer and trl integration with accelerate internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a8bdb-e703-4108-86d8-8f7dfcc0c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_script = \"amp_3_job_fine_tune/distributed_peft_scripts/task_instruction_fine_tuner.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b709d78-cedf-4c39-b275-419268ecd7d1",
   "metadata": {},
   "source": [
    "## Part 0: Install Dependencies\n",
    "\n",
    "Install dependencies for all imports used in this notebook or referenced in the distributed fine-tuning script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ac8f0-3489-4686-9fcd-18e003a2eccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337275b1-51c8-49a7-bec8-a3934303a367",
   "metadata": {},
   "source": [
    "## Part 1: Generate accelerate configuration\n",
    "See https://huggingface.co/docs/accelerate/quicktour for guides on how to manually set up accelerate across workers if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd1e57-4c10-460e-b114-e214b0e51dcc",
   "metadata": {},
   "source": [
    "Must generate configurations for:\n",
    "- NUM_WORKERS : (2) number of separate CML sessions/workers to run\n",
    "- NUM_GPU_PER_WORKER : (1) GPU per CML Worker\n",
    "  - See gpu_ids in accelerate configuration guide to adjust this in your accelerate config template\n",
    "- MASTER_IP : The POD IP of this main CML session\n",
    "\n",
    "These are the main variable configurations for accelerate we are concerned with to control distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b42e40-b56f-4d4a-856d-789a18e5a54a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "NUM_WORKERS = 2\n",
    "NUM_GPU_PER_WORKER = 1\n",
    "MASTER_IP = os.environ[\"CDSW_IP_ADDRESS\"]\n",
    "\n",
    "# Set directory for all sub-workers to pull configurations from\n",
    "conf_dir = \"./.tmp_accelerate_configs_notebook/\"\n",
    "config_path_tmpl = conf_dir + \"${WORKER}_config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc39531",
   "metadata": {},
   "source": [
    "Different accelerate configurations are required for each accelerate worker, set that up here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c6a32-1167-4f6b-8a74-af6cb83fdacf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from string import Template\n",
    "\n",
    "template_file = open(\"amp_3_job_fine_tune/distributed_peft_scripts/common/accelerate_configs/accelerate_multi_config.yaml.tmpl\")\n",
    "template_string = template_file.read()\n",
    "template_file.close()\n",
    "\n",
    "os.makedirs(conf_dir, exist_ok=True)\n",
    "for i in range(NUM_WORKERS):\n",
    "    print(\"creating config %i\" % i)\n",
    "    config_file = Template(template_string)\n",
    "    config_file = config_file.substitute(MACHINE_RANK=i, MAIN_SESSION_IP=MASTER_IP, NUM_MACHINES=NUM_WORKERS, NUM_PROCESSES=NUM_WORKERS)\n",
    "    config_path = Template(config_path_tmpl).substitute(WORKER=i)\n",
    "\n",
    "    new_config = open(config_path, \"w\")\n",
    "    new_config.write(config_file)\n",
    "    new_config.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5be227-ce9c-499c-95e8-db08916fd482",
   "metadata": {},
   "source": [
    "## Part 2: Execute accelerate CLI command on this session and spawned workers\n",
    "**Note:** This session counts as worker 0\n",
    "\n",
    "Using the predefined fine-tuning script, launch distributed fine-tuning by launching accelerate on CML Workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02777a6-b222-4b3e-9580-eecd81cc7e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Command template to launch accross all session/workers\n",
    "command_tmpl = \"accelerate launch --config_file $CONF_PATH $TRAIN_SCRIPT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a3f62-8029-4fdb-9ecd-7cf636a9d613",
   "metadata": {},
   "source": [
    "To launch accelerate training in distributed mode we need to execute accelerate launch as a shell command using specific config files for each \"accelerate worker\".\n",
    "\n",
    "eg. If 2 \"accelerate workers\" are specified then there is a worker locally in this session and we launch an additional CML Worker\n",
    "\n",
    "eg. If 3 \"accelerate workers\" are specified then there is a worker locally in this session and we launch two additional CML Worker and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72303f-ba85-4ae5-9afc-cecf4ef3901e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cml.workers_v1 import launch_workers\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# Picking CPU and MEM profile\n",
    "worker_cpu = 2\n",
    "worker_memory = 8\n",
    "\n",
    "# if changing worker_gpu here, also change gpu_ids in accelerate_multi_config.yaml.tmpl\n",
    "worker_gpu = 1\n",
    "\n",
    "for i in range(NUM_WORKERS):\n",
    "    # Each accelerate launch requires different configuration\n",
    "    config_path = Template(config_path_tmpl).substitute(WORKER=i)\n",
    "    \n",
    "    # See top of notebook for where train_script comes from\n",
    "    command = Template(command_tmpl).substitute(CONF_PATH=config_path, TRAIN_SCRIPT=train_script)\n",
    "\n",
    "    # Wrapping execution into subprocess for convenience in this notebook, but this could be done manually or via CML Jobs\n",
    "    # If worker num 0 this is the main process and should run locally in this session\n",
    "    if i == 0:\n",
    "        print(\"Launch accelerate locally (this session acts as worker of rank 1 aka main worker)...\")\n",
    "        print(\"\\t Command: [%s]\" % command)\n",
    "        main_cmd = subprocess.Popen([f'bash -c \"{command}\" '], shell=True)\n",
    "\n",
    "    # All other accelerate launches will use rank 1+\n",
    "    else:\n",
    "        print((\"Launch CML worker and launch accelerate within them ...\"))\n",
    "        print(\"\\t Command: [%s]\" % command)\n",
    "        launch_workers(name=f'LoRA Train Worker {i}', n=1, cpu=worker_cpu, memory=worker_memory, nvidia_gpu = worker_gpu,  code=\"!\"+command + \" &> /dev/null\")\n",
    "\n",
    "# Waiting for all subworkers to ready up...\n",
    "main_cmd.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a7904",
   "metadata": {},
   "source": [
    "## Done!\n",
    "Your fine-tuned adapter is located in ./amp_adapters_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db84cda",
   "metadata": {},
   "source": [
    "## Part 3: Inference Comparison (Base Model vs Base Model + Adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cc587",
   "metadata": {},
   "source": [
    "### Load base model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b1\", return_dict=True, device_map='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd3cc92",
   "metadata": {},
   "source": [
    "### Load the fine-tuned adapter for use with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = PeftModel.from_pretrained(model=model,                                                 # The base model to load fine-tuned adapters with\n",
    "                                  model_id=\"amp_adapters_custom/bloom1b1-lora-instruct\",       # The directory path of the fine-tuned adapater built in Part 1\n",
    "                                  adapter_name=\"bloom1b1-lora-instruct\",              # A label for this adapter to enable and disable on demand later\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449dbde0",
   "metadata": {},
   "source": [
    "### Define an instruction-following test prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57498fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<Instruction>: Classify the following items into two categories: fruits and vegetables.\n",
    "<Input>: tomato, apple, cucumber, carrot, banana, zucchini, strawberry, cauliflower\n",
    "<Response>:\"\"\"\n",
    "batch = tokenizer(prompt, return_tensors='pt')\n",
    "batch = batch.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b625a",
   "metadata": {},
   "source": [
    "#### Base Model Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9df26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with base model only:\n",
    "\n",
    "with model.disable_adapter():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(**batch, max_new_tokens=60)\n",
    "    prompt_length = len(prompt)\n",
    "    print(tokenizer.decode(output_tokens[0], skip_special_tokens=True)[prompt_length:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c588e",
   "metadata": {},
   "source": [
    "^ The base model shows no ability to follow instructions in the promp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc78eb9",
   "metadata": {},
   "source": [
    "#### Fine-tuned adapter Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbeff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with fine-tuned adapter:\n",
    "model.set_adapter(\"bloom1b1-lora-instruct\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch, max_new_tokens=60)\n",
    "prompt_length = len(prompt)\n",
    "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True)[prompt_length:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b7ffd",
   "metadata": {},
   "source": [
    "^ This is not a perfect response, but a good step towards a usable instruction-following LLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
