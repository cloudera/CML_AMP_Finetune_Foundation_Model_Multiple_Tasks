{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f0187d-ea17-4b2c-ae5d-c41b2c95b040",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Local Fine-Tuning a foundation model for multiple tasks (with QLoRA)\n",
    "The following notebook is an example of performing QLoRA fine-tuning on an LLM using an instruction-following dataset. This script produces the same instruction-following adapter as shown in the amp_adapters_prebuilt directory and the CML Job \"Job for fine-tuning on Instruction Dataset\"\n",
    "\n",
    "Note: This does not run fine-tuning distributed accross multiple CML Workers. That requires launching the huggingface accelerate cli specifying fine-tuning python scripts. See the implementation README in dsitributed_peft_scripts for a description of launching fine-tuning with accelerate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232e713-2dcd-4a48-98b8-f70f4a25be16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 0: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4e7743-47a5-46fc-8693-e996a4ec5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0aebe6",
   "metadata": {},
   "source": [
    "## Part 1: Parameter Efficient Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff26fc5c-7129-4eee-9cfa-94c8f0780a63",
   "metadata": {},
   "source": [
    "### Load the base model with 4bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb4f8e-b49d-4338-b8bb-b48f6d33383a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a48e78",
   "metadata": {},
   "source": [
    "### Load the tokenizer and base model in quantized mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96335985-04f6-4a56-b214-e5a1215d9adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = \"bigscience/bloom-1b1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Configuration to load the model in 4bit quantized mode\n",
    "# bitsandbytes is used for loading the base model we want to fine-tune in nf4 (4-bit normal float as described in the QLoRA paper)\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e1775-0927-4409-9f49-0f20c3e19ef0",
   "metadata": {},
   "source": [
    "### Get Peft Model with LoRA training configuration\n",
    "The peft library from huggingface gives a convenient function to give us a fine-tunable model object where we can specify any lora parameters required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc936d0-dc62-4093-bbe6-4da52fe6b18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "          r=16,\n",
    "          lora_alpha=32,\n",
    "          target_modules=[\"query_key_value\", \"xxx\"],\n",
    "          lora_dropout=0.05,\n",
    "          bias=\"none\",\n",
    "          task_type=\"CAUSAL_LM\"\n",
    "      )\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea22648-5894-410e-b53b-954b040db72b",
   "metadata": {},
   "source": [
    "### Get and modify dataset\n",
    "The dataset we are using for fine-tuning is split up into columns. We need to map each of those example rows into a single string which includes any special tokens we want to use for prompting later for SFTTrainer (from the trl library) to use during fine-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a50cd6-b796-4c42-bfe3-6b3f5396c76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use only 30% of the dataset\n",
    "dataset_fraction = 30\n",
    "data = datasets.load_dataset('teknium/GPTeacher-General-Instruct', split=f'train[:{dataset_fraction}%]')\n",
    "\n",
    "# Merge function to combine two columns of the dataset to have examples that look like\n",
    "#<Instruction>: %s\n",
    "#<Input>: %s\n",
    "#<Response>: %s\n",
    "#    or\n",
    "#<Instruction>: %s\n",
    "#<Response>: %s\n",
    "def merge_columns(example):\n",
    "    if example[\"input\"]:\n",
    "      prediction_format = \"\"\"<Instruction>: %s\n",
    "<Input>: %s\n",
    "<Response>: %s\"\"\"\n",
    "      example[\"prediction\"] = prediction_format %(example[\"instruction\"], example[\"input\"], example[\"response\"])\n",
    "    else:\n",
    "      prediction_format = \"\"\"<Instruction>: %s\n",
    "<Response>: %s\"\"\"\n",
    "      example[\"prediction\"] = prediction_format %(example[\"instruction\"], example[\"response\"])\n",
    "    return example\n",
    "\n",
    "finetuning_data = data.map(merge_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2accba8-67c7-48e7-aa4b-aa94217ab574",
   "metadata": {},
   "source": [
    "### Set up SFTTrainer for PEFT fine-tuning\n",
    "Specify all of the fine-tuning options in TrainingArguments (transformers library) and SFTTrainer (trl library) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bbf88b-f90d-4c27-a17c-5062c893d447",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TrainingArguments from the huggingface transformers library\n",
    "training_args = TrainingArguments(\n",
    "                output_dir=\"outputs\",\n",
    "                num_train_epochs=1,\n",
    "                optim=\"paged_adamw_32bit\",\n",
    "                per_device_train_batch_size=1, \n",
    "                gradient_accumulation_steps=4,\n",
    "                warmup_ratio=0.03, \n",
    "                max_grad_norm=0.3,\n",
    "                learning_rate=2e-4, \n",
    "                fp16=True,\n",
    "                logging_steps=1,\n",
    "                lr_scheduler_type=\"constant\",\n",
    "                disable_tqdm=True,\n",
    "                report_to='tensorboard',\n",
    ")\n",
    "\n",
    "# SFTTrainer from the huggingface trl library\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                       # The model we loaded in quantized mode with lora configuration\n",
    "    train_dataset=finetuning_data,     # The downloaded dataset to use for training\n",
    "    dataset_text_field = \"prediction\", # The column which contains examples used for training\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,                      # Pack multiple fine-tuning examples into the context window for the base-model (cutting down on time to fine-tune)\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c83c3f-7436-4333-9abb-cf093f1294e6",
   "metadata": {},
   "source": [
    "### Launch fine-tuning\n",
    "Fine-tuning takes approximately 14 minutes on a V100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8304b371-02e4-4c76-87f2-a45eb6dd24ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384159a-a6f7-41f5-a2d5-23f10354a494",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save adapter\n",
    "Save the fine-tuned adapter into a directory on disk for loading later during inference time\n",
    "\n",
    "NOTE: sfttrainer savemodel() saves the adapter only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84bb5ddc-0017-4b9d-bb6a-a31a71a76b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"amp_adapters_custom/bloom1b1-lora-instruct-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013c3ce",
   "metadata": {},
   "source": [
    "## Part 2: Inference Comparison (Base Model vs Base Model + Adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76a013-642e-4a57-a364-4cefb01651a1",
   "metadata": {},
   "source": [
    "### Reset CUDA device for inferencing\n",
    "\n",
    "Removing the perviously loaded assets (from Part 1) to free up room on GPU and have a clean place to load resources for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86151a0b-5d83-4ebd-9eb6-6d25b746fa26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del trainer\n",
    "del model\n",
    "del tokenizer\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd02717-07f9-43fc-83a7-78b3794ced51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load base model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a96002-f40b-48c1-ac0c-88db6b86988e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b1\", return_dict=True, device_map='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb8868-be86-4d55-b82b-1e51e60caa9a",
   "metadata": {},
   "source": [
    "### Load the fine-tuned adapter for use with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23415afd-f153-4ab6-b398-859c97d67398",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model=model,                                                 # The base model to load fine-tuned adapters with\n",
    "                                  model_id=\"amp_adapters_custom/bloom1b1-lora-instruct-notebook\",  # The directory path of the fine-tuned adapater built in Part 1\n",
    "                                  adapter_name=\"bloom1b1-lora-instruct-notebook\",              # A label for this adapter to enable and disable on demand later\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0ec4c-9891-41a0-91de-eb3ed9fb91da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define an instruction-following test prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39fa10de-ee6e-4690-8371-758fdb4cbe89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"<Instruction>: Classify the following items into two categories: fruits and vegetables.\n",
    "<Input>: tomato, apple, cucumber, carrot, banana, zucchini, strawberry, cauliflower\n",
    "<Response>:\"\"\"\n",
    "batch = tokenizer(prompt, return_tensors='pt')\n",
    "batch = batch.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6851e5d-c020-4b5f-9e26-5a3578c32d70",
   "metadata": {},
   "source": [
    "#### Base Model Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5af0669-a033-41b1-8fc6-59938246aaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " green, yellow, red, orange, red, yellow, green, blue, yellow, red, orange, red, yellow, green, blue, yellow, red, orange, red, yellow, green, blue, yellow, red, orange, red, yellow, green, blue, yellow,\n"
     ]
    }
   ],
   "source": [
    "# Inference with base model only:\n",
    "\n",
    "with model.disable_adapter():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(**batch, max_new_tokens=60)\n",
    "    prompt_length = len(prompt)\n",
    "    print(tokenizer.decode(output_tokens[0], skip_special_tokens=True)[prompt_length:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483b470",
   "metadata": {},
   "source": [
    "^ The base model shows no ability to follow instructions in the promp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617d949-076e-476e-9300-13a1246f75df",
   "metadata": {},
   "source": [
    "#### Fine-tuned adapter Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed4901b-1fb8-4660-84c7-a80bf5e29e95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fruits: Tomato, Apple, Cucumber, Carrot, Banana, Zucchini, Strawberry, Cauliflower. Vegetables: Tomato, Apple, Cucumber, Carrot, Banana, Zucchini, Strawberry, Cauliflower\n"
     ]
    }
   ],
   "source": [
    "# Inference with fine-tuned adapter:\n",
    "model.set_adapter(\"bloom1b1-lora-instruct-notebook\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch, max_new_tokens=60)\n",
    "prompt_length = len(prompt)\n",
    "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True)[prompt_length:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcea8ab",
   "metadata": {},
   "source": [
    "^ This is not a perfect response, but a good step towards a usable instruction-following LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf8b8a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
